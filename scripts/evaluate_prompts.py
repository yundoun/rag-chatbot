#!/usr/bin/env python3
"""
Prompt Evaluation CLI Tool

Evaluate RAG prompts against test queries and measure quality.

Usage:
    python scripts/evaluate_prompts.py --queries ./data/test_queries.json
    python scripts/evaluate_prompts.py --query "RAG ì‹œìŠ¤í…œì´ë€ ë¬´ì—‡ì¸ê°€ìš”?"
    python scripts/evaluate_prompts.py --benchmark
"""

import argparse
import json
import os
import sys
import time
from pathlib import Path
from typing import Dict, List, Optional

# Add project root to path
sys.path.insert(0, str(Path(__file__).parent.parent))

from dataclasses import dataclass, field
from datetime import datetime


@dataclass
class EvaluationResult:
    """Result of a single query evaluation"""

    query: str
    response: str
    latency_ms: float
    retrieved_docs: int
    relevance_score: Optional[float] = None
    expected_answer: Optional[str] = None
    similarity_score: Optional[float] = None
    error: Optional[str] = None


@dataclass
class EvaluationSummary:
    """Summary of evaluation results"""

    total_queries: int = 0
    successful_queries: int = 0
    failed_queries: int = 0
    avg_latency_ms: float = 0.0
    avg_relevance_score: float = 0.0
    avg_similarity_score: float = 0.0
    results: List[EvaluationResult] = field(default_factory=list)


class PromptEvaluator:
    """Evaluates RAG prompts and responses"""

    def __init__(self, collection_name: str = "rag-documents"):
        self.collection_name = collection_name
        self._llm = None
        self._embeddings = None
        self._retriever = None

    def _init_components(self):
        """Initialize LLM and retriever"""
        try:
            from langchain_openai import ChatOpenAI, OpenAIEmbeddings
            import chromadb
            from chromadb.config import Settings

            api_key = os.environ.get("OPENAI_API_KEY")
            if not api_key:
                print("âŒ OPENAI_API_KEY í™˜ê²½ ë³€ìˆ˜ê°€ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
                sys.exit(1)

            self._llm = ChatOpenAI(
                model="gpt-4o-mini",
                temperature=0,
                openai_api_key=api_key,
            )

            self._embeddings = OpenAIEmbeddings(
                model="text-embedding-3-small",
                openai_api_key=api_key,
            )

            # Initialize ChromaDB
            client = chromadb.PersistentClient(
                path="./data/chroma",
                settings=Settings(anonymized_telemetry=False),
            )

            try:
                self._collection = client.get_collection(self.collection_name)
                print(f"âœ… ì»¬ë ‰ì…˜ ë¡œë“œ ì™„ë£Œ: {self.collection_name}")
                print(f"   ë¬¸ì„œ ìˆ˜: {self._collection.count()}")
            except Exception:
                print(f"âš ï¸ ì»¬ë ‰ì…˜ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {self.collection_name}")
                print("   ë¨¼ì € index_documents.pyë¥¼ ì‹¤í–‰í•´ ì£¼ì„¸ìš”.")
                self._collection = None

            print("âœ… ì»´í¬ë„ŒíŠ¸ ì´ˆê¸°í™” ì™„ë£Œ")

        except ImportError as e:
            print(f"âŒ í•„ìš”í•œ íŒ¨í‚¤ì§€ê°€ ì„¤ì¹˜ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤: {e}")
            sys.exit(1)

    def _retrieve(self, query: str, top_k: int = 5) -> List[dict]:
        """Retrieve relevant documents"""
        if self._collection is None:
            return []

        try:
            query_embedding = self._embeddings.embed_query(query)

            results = self._collection.query(
                query_embeddings=[query_embedding],
                n_results=top_k,
                include=["documents", "metadatas", "distances"],
            )

            docs = []
            if results and results["documents"]:
                for i, doc in enumerate(results["documents"][0]):
                    docs.append(
                        {
                            "content": doc,
                            "metadata": (
                                results["metadatas"][0][i]
                                if results["metadatas"]
                                else {}
                            ),
                            "distance": (
                                results["distances"][0][i]
                                if results["distances"]
                                else 0
                            ),
                        }
                    )
            return docs

        except Exception as e:
            print(f"  âš ï¸ ê²€ìƒ‰ ì˜¤ë¥˜: {e}")
            return []

    def _generate_response(self, query: str, context: str) -> str:
        """Generate response using LLM"""
        prompt = f"""ë‹¤ìŒ ì»¨í…ìŠ¤íŠ¸ë¥¼ ê¸°ë°˜ìœ¼ë¡œ ì§ˆë¬¸ì— ë‹µë³€í•´ ì£¼ì„¸ìš”.

ì»¨í…ìŠ¤íŠ¸:
{context}

ì§ˆë¬¸: {query}

ë‹µë³€:"""

        try:
            response = self._llm.invoke(prompt)
            return response.content
        except Exception as e:
            return f"ì˜¤ë¥˜: {e}"

    def _calculate_similarity(self, text1: str, text2: str) -> float:
        """Calculate semantic similarity between two texts"""
        try:
            emb1 = self._embeddings.embed_query(text1)
            emb2 = self._embeddings.embed_query(text2)

            # Cosine similarity
            import math

            dot_product = sum(a * b for a, b in zip(emb1, emb2))
            norm1 = math.sqrt(sum(a * a for a in emb1))
            norm2 = math.sqrt(sum(b * b for b in emb2))

            if norm1 == 0 or norm2 == 0:
                return 0.0

            return dot_product / (norm1 * norm2)
        except Exception:
            return 0.0

    def evaluate_query(
        self,
        query: str,
        expected_answer: Optional[str] = None,
        top_k: int = 5,
    ) -> EvaluationResult:
        """Evaluate a single query"""
        start_time = time.perf_counter()

        try:
            # Retrieve documents
            docs = self._retrieve(query, top_k)

            if not docs:
                return EvaluationResult(
                    query=query,
                    response="",
                    latency_ms=0,
                    retrieved_docs=0,
                    error="ê²€ìƒ‰ ê²°ê³¼ ì—†ìŒ",
                )

            # Build context
            context = "\n\n".join([d["content"] for d in docs])

            # Calculate average relevance score
            avg_relevance = 1 - (sum(d["distance"] for d in docs) / len(docs))

            # Generate response
            response = self._generate_response(query, context)

            latency_ms = (time.perf_counter() - start_time) * 1000

            result = EvaluationResult(
                query=query,
                response=response,
                latency_ms=latency_ms,
                retrieved_docs=len(docs),
                relevance_score=avg_relevance,
                expected_answer=expected_answer,
            )

            # Calculate similarity if expected answer provided
            if expected_answer:
                result.similarity_score = self._calculate_similarity(
                    response, expected_answer
                )

            return result

        except Exception as e:
            return EvaluationResult(
                query=query,
                response="",
                latency_ms=(time.perf_counter() - start_time) * 1000,
                retrieved_docs=0,
                error=str(e),
            )

    def evaluate_queries(
        self,
        queries: List[Dict],
        top_k: int = 5,
    ) -> EvaluationSummary:
        """Evaluate multiple queries"""
        self._init_components()

        summary = EvaluationSummary()
        summary.total_queries = len(queries)

        print(f"\nğŸ” {len(queries)}ê°œ ì¿¼ë¦¬ í‰ê°€ ì‹œì‘")
        print("=" * 50)

        for i, query_data in enumerate(queries, 1):
            query = query_data.get("query", query_data) if isinstance(query_data, dict) else query_data
            expected = query_data.get("expected") if isinstance(query_data, dict) else None

            print(f"\n[{i}/{len(queries)}] í‰ê°€ ì¤‘: {query[:50]}...")

            result = self.evaluate_query(query, expected, top_k)
            summary.results.append(result)

            if result.error:
                summary.failed_queries += 1
                print(f"  âŒ ì‹¤íŒ¨: {result.error}")
            else:
                summary.successful_queries += 1
                print(f"  âœ… ì„±ê³µ - ì§€ì—°: {result.latency_ms:.0f}ms, ë¬¸ì„œ: {result.retrieved_docs}")
                if result.relevance_score:
                    print(f"     ê´€ë ¨ì„±: {result.relevance_score:.2f}")
                if result.similarity_score:
                    print(f"     ìœ ì‚¬ë„: {result.similarity_score:.2f}")

        # Calculate averages
        successful = [r for r in summary.results if not r.error]
        if successful:
            summary.avg_latency_ms = sum(r.latency_ms for r in successful) / len(successful)

            relevance_scores = [r.relevance_score for r in successful if r.relevance_score]
            if relevance_scores:
                summary.avg_relevance_score = sum(relevance_scores) / len(relevance_scores)

            similarity_scores = [r.similarity_score for r in successful if r.similarity_score]
            if similarity_scores:
                summary.avg_similarity_score = sum(similarity_scores) / len(similarity_scores)

        return summary

    def run_benchmark(self) -> EvaluationSummary:
        """Run benchmark with predefined test queries"""
        benchmark_queries = [
            {
                "query": "RAG ì‹œìŠ¤í…œì˜ ì£¼ìš” êµ¬ì„± ìš”ì†ŒëŠ” ë¬´ì—‡ì¸ê°€ìš”?",
                "expected": "RAG ì‹œìŠ¤í…œì€ ê²€ìƒ‰(Retrieval), ì¦ê°•(Augmentation), ìƒì„±(Generation) ì„¸ ê°€ì§€ ì£¼ìš” êµ¬ì„± ìš”ì†Œë¡œ ì´ë£¨ì–´ì ¸ ìˆìŠµë‹ˆë‹¤.",
            },
            {
                "query": "ë²¡í„° ë°ì´í„°ë² ì´ìŠ¤ë€ ë¬´ì—‡ì¸ê°€ìš”?",
                "expected": "ë²¡í„° ë°ì´í„°ë² ì´ìŠ¤ëŠ” ê³ ì°¨ì› ë²¡í„° ë°ì´í„°ë¥¼ ì €ì¥í•˜ê³  ìœ ì‚¬ì„± ê²€ìƒ‰ì„ ìˆ˜í–‰í•˜ëŠ” ë° ìµœì í™”ëœ ë°ì´í„°ë² ì´ìŠ¤ì…ë‹ˆë‹¤.",
            },
            {
                "query": "ì„ë² ë”©ì´ë€ ë¬´ì—‡ì¸ê°€ìš”?",
                "expected": "ì„ë² ë”©ì€ í…ìŠ¤íŠ¸ë‚˜ ë‹¤ë¥¸ ë°ì´í„°ë¥¼ ê³ ì°¨ì› ë²¡í„° ê³µê°„ì— ë§¤í•‘í•˜ëŠ” ê²ƒìœ¼ë¡œ, ì˜ë¯¸ì  ìœ ì‚¬ì„±ì„ ìˆ˜ì¹˜ë¡œ í‘œí˜„í•©ë‹ˆë‹¤.",
            },
            {
                "query": "LangChainì˜ ì£¼ìš” ê¸°ëŠ¥ì€?",
                "expected": "LangChainì€ LLM ì• í”Œë¦¬ì¼€ì´ì…˜ ê°œë°œì„ ìœ„í•œ í”„ë ˆì„ì›Œí¬ë¡œ, ì²´ì¸, ì—ì´ì „íŠ¸, ë©”ëª¨ë¦¬ ë“±ì˜ ê¸°ëŠ¥ì„ ì œê³µí•©ë‹ˆë‹¤.",
            },
            {
                "query": "í”„ë¡¬í”„íŠ¸ ì—”ì§€ë‹ˆì–´ë§ì´ë€?",
                "expected": "í”„ë¡¬í”„íŠ¸ ì—”ì§€ë‹ˆì–´ë§ì€ LLMì—ì„œ ì›í•˜ëŠ” ì¶œë ¥ì„ ì–»ê¸° ìœ„í•´ ì…ë ¥ í”„ë¡¬í”„íŠ¸ë¥¼ ì„¤ê³„í•˜ê³  ìµœì í™”í•˜ëŠ” ê¸°ìˆ ì…ë‹ˆë‹¤.",
            },
        ]

        print("\nğŸ ë²¤ì¹˜ë§ˆí¬ ì‹¤í–‰")
        return self.evaluate_queries(benchmark_queries)


def print_summary(summary: EvaluationSummary):
    """Print evaluation summary"""
    print("\n" + "=" * 50)
    print("ğŸ“Š í‰ê°€ ê²°ê³¼ ìš”ì•½")
    print("=" * 50)
    print(f"  ì´ ì¿¼ë¦¬ ìˆ˜: {summary.total_queries}")
    print(f"  ì„±ê³µ: {summary.successful_queries}")
    print(f"  ì‹¤íŒ¨: {summary.failed_queries}")
    print(f"  í‰ê·  ì§€ì—° ì‹œê°„: {summary.avg_latency_ms:.0f}ms")
    print(f"  í‰ê·  ê´€ë ¨ì„± ì ìˆ˜: {summary.avg_relevance_score:.2f}")

    if summary.avg_similarity_score > 0:
        print(f"  í‰ê·  ìœ ì‚¬ë„ ì ìˆ˜: {summary.avg_similarity_score:.2f}")

    # Quality assessment
    print("\nğŸ“ˆ í’ˆì§ˆ í‰ê°€:")
    if summary.avg_relevance_score >= 0.8:
        print("  âœ… ê´€ë ¨ì„±: ìš°ìˆ˜")
    elif summary.avg_relevance_score >= 0.6:
        print("  âš ï¸ ê´€ë ¨ì„±: ì–‘í˜¸")
    else:
        print("  âŒ ê´€ë ¨ì„±: ê°œì„  í•„ìš”")

    if summary.avg_latency_ms <= 2000:
        print("  âœ… ì‘ë‹µ ì†ë„: ìš°ìˆ˜")
    elif summary.avg_latency_ms <= 5000:
        print("  âš ï¸ ì‘ë‹µ ì†ë„: ì–‘í˜¸")
    else:
        print("  âŒ ì‘ë‹µ ì†ë„: ê°œì„  í•„ìš”")


def save_results(summary: EvaluationSummary, output_path: str):
    """Save evaluation results to JSON"""
    results = {
        "timestamp": datetime.utcnow().isoformat(),
        "summary": {
            "total_queries": summary.total_queries,
            "successful_queries": summary.successful_queries,
            "failed_queries": summary.failed_queries,
            "avg_latency_ms": summary.avg_latency_ms,
            "avg_relevance_score": summary.avg_relevance_score,
            "avg_similarity_score": summary.avg_similarity_score,
        },
        "results": [
            {
                "query": r.query,
                "response": r.response,
                "latency_ms": r.latency_ms,
                "retrieved_docs": r.retrieved_docs,
                "relevance_score": r.relevance_score,
                "similarity_score": r.similarity_score,
                "expected_answer": r.expected_answer,
                "error": r.error,
            }
            for r in summary.results
        ],
    }

    with open(output_path, "w", encoding="utf-8") as f:
        json.dump(results, f, ensure_ascii=False, indent=2)

    print(f"\nğŸ’¾ ê²°ê³¼ ì €ì¥ë¨: {output_path}")


def main():
    parser = argparse.ArgumentParser(
        description="í”„ë¡¬í”„íŠ¸ í‰ê°€ CLI ë„êµ¬",
        formatter_class=argparse.RawDescriptionHelpFormatter,
        epilog="""
ì˜ˆì‹œ:
  python scripts/evaluate_prompts.py --benchmark
  python scripts/evaluate_prompts.py --query "RAGë€ ë¬´ì—‡ì¸ê°€ìš”?"
  python scripts/evaluate_prompts.py --queries ./test_queries.json --output ./results.json
        """,
    )

    parser.add_argument(
        "--query",
        type=str,
        help="ë‹¨ì¼ ì¿¼ë¦¬ í‰ê°€",
    )
    parser.add_argument(
        "--queries",
        type=str,
        help="ì¿¼ë¦¬ íŒŒì¼ ê²½ë¡œ (JSON)",
    )
    parser.add_argument(
        "--collection",
        type=str,
        default="rag-documents",
        help="ChromaDB ì»¬ë ‰ì…˜ ì´ë¦„",
    )
    parser.add_argument(
        "--top-k",
        type=int,
        default=5,
        help="ê²€ìƒ‰í•  ë¬¸ì„œ ìˆ˜ (ê¸°ë³¸: 5)",
    )
    parser.add_argument(
        "--benchmark",
        action="store_true",
        help="ë²¤ì¹˜ë§ˆí¬ ì‹¤í–‰",
    )
    parser.add_argument(
        "--output",
        type=str,
        help="ê²°ê³¼ ì €ì¥ ê²½ë¡œ (JSON)",
    )

    args = parser.parse_args()

    evaluator = PromptEvaluator(collection_name=args.collection)

    if args.benchmark:
        summary = evaluator.run_benchmark()
    elif args.query:
        evaluator._init_components()
        print(f"\nğŸ” ì¿¼ë¦¬ í‰ê°€: {args.query}")
        result = evaluator.evaluate_query(args.query, top_k=args.top_k)

        if result.error:
            print(f"âŒ ì˜¤ë¥˜: {result.error}")
        else:
            print(f"\nğŸ“ ì‘ë‹µ:\n{result.response}")
            print(f"\nâ±ï¸ ì§€ì—° ì‹œê°„: {result.latency_ms:.0f}ms")
            print(f"ğŸ“š ê²€ìƒ‰ëœ ë¬¸ì„œ: {result.retrieved_docs}")
            if result.relevance_score:
                print(f"ğŸ“Š ê´€ë ¨ì„± ì ìˆ˜: {result.relevance_score:.2f}")
        return
    elif args.queries:
        with open(args.queries, "r", encoding="utf-8") as f:
            queries = json.load(f)
        summary = evaluator.evaluate_queries(queries, top_k=args.top_k)
    else:
        parser.error("--query, --queries, ë˜ëŠ” --benchmark ì˜µì…˜ì´ í•„ìš”í•©ë‹ˆë‹¤")

    print_summary(summary)

    if args.output:
        save_results(summary, args.output)


if __name__ == "__main__":
    main()
