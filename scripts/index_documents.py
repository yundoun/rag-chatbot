#!/usr/bin/env python3
"""
Document Indexing CLI Tool

Batch index markdown documents into ChromaDB vector store.

Usage:
    python scripts/index_documents.py --source ./data/docs --collection rag-docs
    python scripts/index_documents.py --source ./data/docs --reset
    python scripts/index_documents.py --status
"""

import argparse
import hashlib
import os
import sys
from pathlib import Path
from typing import List, Optional

# Add project root to path
sys.path.insert(0, str(Path(__file__).parent.parent))

from dataclasses import dataclass
from datetime import datetime


@dataclass
class DocumentChunk:
    """Represents a document chunk for indexing"""

    content: str
    metadata: dict
    doc_id: str


@dataclass
class IndexingStats:
    """Statistics for indexing operation"""

    total_files: int = 0
    processed_files: int = 0
    total_chunks: int = 0
    skipped_files: int = 0
    errors: List[str] = None

    def __post_init__(self):
        if self.errors is None:
            self.errors = []


class DocumentIndexer:
    """Indexes markdown documents into ChromaDB"""

    def __init__(
        self,
        collection_name: str = "documents",
        chunk_size: int = 1000,
        chunk_overlap: int = 200,
        persist_directory: str = "./data/chroma_db",
    ):
        self.collection_name = collection_name
        self.chunk_size = chunk_size
        self.chunk_overlap = chunk_overlap
        self.persist_directory = persist_directory
        self._client = None
        self._collection = None
        self._embeddings = None

    def _init_chromadb(self):
        """Initialize ChromaDB client"""
        try:
            import chromadb
            from chromadb.config import Settings

            self._client = chromadb.PersistentClient(
                path=self.persist_directory,
                settings=Settings(anonymized_telemetry=False),
            )
            self._collection = self._client.get_or_create_collection(
                name=self.collection_name,
                metadata={"hnsw:space": "cosine"},
            )
            print(f"âœ… ChromaDB ì´ˆê¸°í™” ì™„ë£Œ: {self.collection_name}")
        except ImportError:
            print("âŒ chromadb íŒ¨í‚¤ì§€ê°€ ì„¤ì¹˜ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
            print("   pip install chromadb")
            sys.exit(1)
        except Exception as e:
            print(f"âŒ ChromaDB ì´ˆê¸°í™” ì‹¤íŒ¨: {e}")
            sys.exit(1)

    def _init_embeddings(self):
        """Initialize embedding model"""
        try:
            from langchain_openai import OpenAIEmbeddings

            api_key = os.environ.get("OPENAI_API_KEY")
            if not api_key:
                print("âŒ OPENAI_API_KEY í™˜ê²½ ë³€ìˆ˜ê°€ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
                sys.exit(1)

            self._embeddings = OpenAIEmbeddings(
                model="text-embedding-3-small",
                openai_api_key=api_key,
            )
            print("âœ… ì„ë² ë”© ëª¨ë¸ ì´ˆê¸°í™” ì™„ë£Œ")
        except ImportError:
            print("âŒ langchain-openai íŒ¨í‚¤ì§€ê°€ ì„¤ì¹˜ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
            print("   pip install langchain-openai")
            sys.exit(1)
        except Exception as e:
            print(f"âŒ ì„ë² ë”© ëª¨ë¸ ì´ˆê¸°í™” ì‹¤íŒ¨: {e}")
            sys.exit(1)

    def _read_markdown_file(self, file_path: Path) -> Optional[str]:
        """Read markdown file content"""
        try:
            with open(file_path, "r", encoding="utf-8") as f:
                return f.read()
        except Exception as e:
            print(f"  âš ï¸ íŒŒì¼ ì½ê¸° ì‹¤íŒ¨: {file_path} - {e}")
            return None

    def _chunk_text(self, text: str, file_path: Path) -> List[DocumentChunk]:
        """Split text into chunks with metadata"""
        chunks = []
        file_name = file_path.stem
        file_hash = hashlib.md5(str(file_path).encode()).hexdigest()[:8]

        # Simple chunking by paragraphs first, then by size
        paragraphs = text.split("\n\n")
        current_chunk = ""
        chunk_index = 0

        for para in paragraphs:
            if len(current_chunk) + len(para) + 2 <= self.chunk_size:
                current_chunk += para + "\n\n"
            else:
                if current_chunk.strip():
                    chunks.append(
                        self._create_chunk(
                            current_chunk.strip(),
                            file_path,
                            file_hash,
                            chunk_index,
                        )
                    )
                    chunk_index += 1

                # Handle large paragraphs
                if len(para) > self.chunk_size:
                    # Split by sentences or fixed size
                    for i in range(0, len(para), self.chunk_size - self.chunk_overlap):
                        chunk_text = para[i : i + self.chunk_size]
                        if chunk_text.strip():
                            chunks.append(
                                self._create_chunk(
                                    chunk_text.strip(),
                                    file_path,
                                    file_hash,
                                    chunk_index,
                                )
                            )
                            chunk_index += 1
                    current_chunk = ""
                else:
                    current_chunk = para + "\n\n"

        # Don't forget the last chunk
        if current_chunk.strip():
            chunks.append(
                self._create_chunk(
                    current_chunk.strip(),
                    file_path,
                    file_hash,
                    chunk_index,
                )
            )

        return chunks

    def _create_chunk(
        self,
        content: str,
        file_path: Path,
        file_hash: str,
        chunk_index: int,
    ) -> DocumentChunk:
        """Create a document chunk with metadata"""
        doc_id = f"{file_hash}_{chunk_index}"

        return DocumentChunk(
            content=content,
            metadata={
                "source": str(file_path),
                "file_name": file_path.name,
                "chunk_index": chunk_index,
                "indexed_at": datetime.utcnow().isoformat(),
            },
            doc_id=doc_id,
        )

    def _get_embeddings(self, texts: List[str]) -> List[List[float]]:
        """Get embeddings for texts"""
        try:
            return self._embeddings.embed_documents(texts)
        except Exception as e:
            print(f"  âš ï¸ ì„ë² ë”© ìƒì„± ì‹¤íŒ¨: {e}")
            return []

    def index_directory(
        self,
        source_dir: str,
        reset: bool = False,
        batch_size: int = 100,
    ) -> IndexingStats:
        """
        Index all markdown files in directory.

        Args:
            source_dir: Directory containing markdown files
            reset: Whether to reset collection before indexing
            batch_size: Number of documents to process at once

        Returns:
            Indexing statistics
        """
        stats = IndexingStats()
        source_path = Path(source_dir)

        if not source_path.exists():
            print(f"âŒ ë””ë ‰í† ë¦¬ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {source_dir}")
            return stats

        # Initialize components
        self._init_chromadb()
        self._init_embeddings()

        # Reset collection if requested
        if reset:
            print("ğŸ—‘ï¸ ê¸°ì¡´ ì»¬ë ‰ì…˜ ì‚­ì œ ì¤‘...")
            try:
                self._client.delete_collection(self.collection_name)
                self._collection = self._client.create_collection(
                    name=self.collection_name,
                    metadata={"hnsw:space": "cosine"},
                )
                print("âœ… ì»¬ë ‰ì…˜ ì´ˆê¸°í™” ì™„ë£Œ")
            except Exception as e:
                print(f"âš ï¸ ì»¬ë ‰ì…˜ ì´ˆê¸°í™” ì¤‘ ì˜¤ë¥˜: {e}")

        # Find markdown files
        md_files = list(source_path.glob("**/*.md"))
        stats.total_files = len(md_files)

        if not md_files:
            print(f"âš ï¸ ë§ˆí¬ë‹¤ìš´ íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {source_dir}")
            return stats

        print(f"\nğŸ“ {stats.total_files}ê°œì˜ ë§ˆí¬ë‹¤ìš´ íŒŒì¼ ë°œê²¬")
        print("=" * 50)

        all_chunks: List[DocumentChunk] = []

        # Process each file
        for i, file_path in enumerate(md_files, 1):
            print(f"\n[{i}/{stats.total_files}] ì²˜ë¦¬ ì¤‘: {file_path.name}")

            content = self._read_markdown_file(file_path)
            if content is None:
                stats.skipped_files += 1
                stats.errors.append(f"ì½ê¸° ì‹¤íŒ¨: {file_path}")
                continue

            # Skip empty files
            if not content.strip():
                print(f"  â­ï¸ ë¹ˆ íŒŒì¼ ê±´ë„ˆëœ€")
                stats.skipped_files += 1
                continue

            chunks = self._chunk_text(content, file_path)
            print(f"  ğŸ“„ {len(chunks)}ê°œ ì²­í¬ ìƒì„±")

            all_chunks.extend(chunks)
            stats.processed_files += 1

        # Index chunks in batches
        if all_chunks:
            print(f"\nğŸ”„ {len(all_chunks)}ê°œ ì²­í¬ ì¸ë±ì‹± ì¤‘...")
            print("=" * 50)

            for i in range(0, len(all_chunks), batch_size):
                batch = all_chunks[i : i + batch_size]
                batch_num = i // batch_size + 1
                total_batches = (len(all_chunks) + batch_size - 1) // batch_size

                print(f"  ë°°ì¹˜ [{batch_num}/{total_batches}] ì²˜ë¦¬ ì¤‘...")

                try:
                    texts = [c.content for c in batch]
                    embeddings = self._get_embeddings(texts)

                    if embeddings:
                        self._collection.add(
                            ids=[c.doc_id for c in batch],
                            documents=texts,
                            embeddings=embeddings,
                            metadatas=[c.metadata for c in batch],
                        )
                        stats.total_chunks += len(batch)
                        print(f"    âœ… {len(batch)}ê°œ ì²­í¬ ì¸ë±ì‹± ì™„ë£Œ")
                    else:
                        print(f"    âš ï¸ ì„ë² ë”© ìƒì„± ì‹¤íŒ¨ë¡œ ë°°ì¹˜ ê±´ë„ˆëœ€")
                        stats.errors.append(f"ë°°ì¹˜ {batch_num} ì„ë² ë”© ì‹¤íŒ¨")

                except Exception as e:
                    print(f"    âŒ ë°°ì¹˜ ì¸ë±ì‹± ì‹¤íŒ¨: {e}")
                    stats.errors.append(f"ë°°ì¹˜ {batch_num} ì¸ë±ì‹± ì‹¤íŒ¨: {e}")

        return stats

    def get_status(self) -> dict:
        """Get collection status"""
        self._init_chromadb()

        try:
            count = self._collection.count()
            return {
                "collection_name": self.collection_name,
                "document_count": count,
                "persist_directory": self.persist_directory,
            }
        except Exception as e:
            return {"error": str(e)}


def print_stats(stats: IndexingStats):
    """Print indexing statistics"""
    print("\n" + "=" * 50)
    print("ğŸ“Š ì¸ë±ì‹± ê²°ê³¼")
    print("=" * 50)
    print(f"  ì´ íŒŒì¼ ìˆ˜: {stats.total_files}")
    print(f"  ì²˜ë¦¬ëœ íŒŒì¼: {stats.processed_files}")
    print(f"  ê±´ë„ˆë›´ íŒŒì¼: {stats.skipped_files}")
    print(f"  ìƒì„±ëœ ì²­í¬: {stats.total_chunks}")

    if stats.errors:
        print(f"\nâš ï¸ ì˜¤ë¥˜ ({len(stats.errors)}ê±´):")
        for error in stats.errors[:5]:
            print(f"  - {error}")
        if len(stats.errors) > 5:
            print(f"  ... ì™¸ {len(stats.errors) - 5}ê±´")


def main():
    parser = argparse.ArgumentParser(
        description="ë¬¸ì„œ ì¸ë±ì‹± CLI ë„êµ¬",
        formatter_class=argparse.RawDescriptionHelpFormatter,
        epilog="""
ì˜ˆì‹œ:
  python scripts/index_documents.py --source ./data/docs
  python scripts/index_documents.py --source ./data/docs --reset
  python scripts/index_documents.py --status
        """,
    )

    parser.add_argument(
        "--source",
        type=str,
        help="ë§ˆí¬ë‹¤ìš´ ë¬¸ì„œ ë””ë ‰í† ë¦¬ ê²½ë¡œ",
    )
    parser.add_argument(
        "--collection",
        type=str,
        default="documents",
        help="ChromaDB ì»¬ë ‰ì…˜ ì´ë¦„ (ê¸°ë³¸: documents)",
    )
    parser.add_argument(
        "--persist-dir",
        type=str,
        default="./data/chroma_db",
        help="ChromaDB ì €ì¥ ê²½ë¡œ (ê¸°ë³¸: ./data/chroma_db)",
    )
    parser.add_argument(
        "--chunk-size",
        type=int,
        default=1000,
        help="ì²­í¬ í¬ê¸° (ê¸°ë³¸: 1000ì)",
    )
    parser.add_argument(
        "--chunk-overlap",
        type=int,
        default=200,
        help="ì²­í¬ ì˜¤ë²„ë© (ê¸°ë³¸: 200ì)",
    )
    parser.add_argument(
        "--batch-size",
        type=int,
        default=100,
        help="ë°°ì¹˜ í¬ê¸° (ê¸°ë³¸: 100)",
    )
    parser.add_argument(
        "--reset",
        action="store_true",
        help="ì¸ë±ì‹± ì „ ì»¬ë ‰ì…˜ ì´ˆê¸°í™”",
    )
    parser.add_argument(
        "--status",
        action="store_true",
        help="ì»¬ë ‰ì…˜ ìƒíƒœ í™•ì¸",
    )

    args = parser.parse_args()

    indexer = DocumentIndexer(
        collection_name=args.collection,
        chunk_size=args.chunk_size,
        chunk_overlap=args.chunk_overlap,
        persist_directory=args.persist_dir,
    )

    if args.status:
        print("\nğŸ“Š ì»¬ë ‰ì…˜ ìƒíƒœ")
        print("=" * 50)
        status = indexer.get_status()
        for key, value in status.items():
            print(f"  {key}: {value}")
        return

    if not args.source:
        parser.error("--source ë˜ëŠ” --status ì˜µì…˜ì´ í•„ìš”í•©ë‹ˆë‹¤")

    print("\nğŸš€ ë¬¸ì„œ ì¸ë±ì‹± ì‹œì‘")
    print("=" * 50)
    print(f"  ì†ŒìŠ¤ ë””ë ‰í† ë¦¬: {args.source}")
    print(f"  ì»¬ë ‰ì…˜: {args.collection}")
    print(f"  ì²­í¬ í¬ê¸°: {args.chunk_size}")
    print(f"  ì´ˆê¸°í™” ëª¨ë“œ: {'ì˜ˆ' if args.reset else 'ì•„ë‹ˆì˜¤'}")

    stats = indexer.index_directory(
        source_dir=args.source,
        reset=args.reset,
        batch_size=args.batch_size,
    )

    print_stats(stats)

    if stats.total_chunks > 0:
        print("\nâœ… ì¸ë±ì‹± ì™„ë£Œ!")
    else:
        print("\nâš ï¸ ì¸ë±ì‹±ëœ ë¬¸ì„œê°€ ì—†ìŠµë‹ˆë‹¤.")


if __name__ == "__main__":
    main()
